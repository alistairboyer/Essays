{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c324f83d-168b-4a75-80dd-939617588e78",
   "metadata": {},
   "source": [
    "# Extracting Data from a PDF Table: AI Ethics Policies\n",
    "# Part 2: The State of the Art of PDF Table Extraction\n",
    "\n",
    "_Alistair Boyer_\n",
    "\n",
    "Close your eyes and imagine a world where every dataset is nicely curated. \n",
    "There is a detailed data dictionary that explains the purpose of each column and all the values that could ever be contained within.\n",
    "Each column has an optimal type associated with the data that provides convenient methods to access the various properties of the data. \n",
    "There are no duplicates across rows. There are no ambiguity across values. \n",
    "Now, __wake up__ because we all know that this is not the world we live in.\n",
    "Working with data most often involves spending a disproportionate amount of time extracting and cleaning data from frustrating sources. \n",
    "\n",
    "\n",
    "A clear contender for the worst offender in the category of swallowing valuable time is tabular data stored in PDF files.\n",
    "These portable files that are super convenient for presenting formatted information and dominate the document landscape.\n",
    "In 2015, [Adobe's Phil Ydens revealed](https://youtu.be/5Axw6OGPYHw) that >70 million new PDFs were saved every day in the Google Cloud; and simple maths extrapolates the total number of PDFs to be well over 2 trillion. \n",
    "However, all this portability comes at a heavy cost. The data within a PDF file is stored as an almost random selection of texts and shapes and their properties and coordinates within each page. \n",
    "We have all experienced the actuality of this when dragging across text within a PDF page to find various words and phrases selected in a random order including page number and headers.\n",
    "What makes extracting data from a PDF even more frustrating is the knowledge that this exact same data exists somewhere in a much more convenient form that someone has decided not to share. \n",
    "\n",
    "I have been reviewing AI Ethics policies and came across the 2019 [perspective article](https://doi.org/10.1038/s42256-019-0088-2) in _Nature Machine Intelligence_ by Anna Jobin, Marcello Ienca and Effy Vayena, titled \"The Global Landscape of AI Ethics Guidelines\" [__2019__, _Vol 1_, pp 389-399].\n",
    "The article covers principles and guidelines for ethical artificial intelligence published across the globe and includes a list of these policies as Table S2 within the [PDF Supplementary Information](https://www.nature.com/articles/s42256-019-0088-2#Sec17). \n",
    "\n",
    "\n",
    "This two-part article describes my elongated journey into this challenge and covers the state-of-the-art for PDF extraction in May 2024.\n",
    "[__Part 1__](RateLimiting_CompaniesHouse_Part1.ipynb) starts with a custom approach to table extraction using __pypdf__ to match all text within the rectangles that make up the table and building an automated response to getting the data.\n",
    "[__Part 2__](RateLimiting_CompaniesHouse_Part2.ipynb) looks at the python packages that are designed for tabular data extraction, starting with __Adobe__'s API for table extraction that reinforces their PDF experience with ML and AI techniques.\n",
    "Then looking at a smorgasbord of python packages: __pdfplumber__, __tabula__ and __camelot__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e174dc85-f581-48b1-8abe-88a96a858212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac84861c-8552-4e35-bff1-d410dc1c4930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pathlib\n",
    "\n",
    "# The SI data is available from the publisher; and is saved locally to GlobalLandscapeOfAIEthicsSI.pdf\n",
    "\n",
    "PDF_URL = \"https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-019-0088-2/MediaObjects/42256_2019_88_MOESM1_ESM.pdf\"\n",
    "\n",
    "PDF = pathlib.Path(\"data/GlobalLandscapeOfAIEthicsSI.pdf\")\n",
    "\n",
    "if not PDF.is_file():\n",
    "    with PDF.open(\"wb\") as f:\n",
    "        f.write(requests.get(PDF_URL).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fedd53-67ac-4fb9-9279-df54dba99377",
   "metadata": {},
   "source": [
    "## PDF Data\n",
    "\n",
    "A PDF file is a collection of text and graphics information and the information required to present this on a page, _i.e._ location, font, transformation, kerning, etc&hellip;.\n",
    "A significant challenge is that unlike something like HTML where the structure of the file reflects the structure of the document, in PDF there is __no guarantee__ that order of data is correlated with its location on a page. \n",
    "There is an excellent description of PDF text operations in this [free book chapter by _Ryan Hodson_](https://www.syncfusion.com/succinctly-free-ebooks/pdf/text-operators)\n",
    "and a handy visual cheat-sheet of PDF operators created [by the PDF Association](https://pdfa.org/wp-content/uploads/2023/08/PDF-Operators-CheatSheet.pdf).\n",
    "\n",
    "\n",
    "However, you don't have to wade through the complex PDF file by yourself and there are many packages that have been created to streamline this process. These packages use various strategies for table extraction including: the same line/rectangle finding strategy as above; to grouping text by position alone; and AI and machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33b293-7585-4b05-b206-b47dcf9a01ed",
   "metadata": {},
   "source": [
    "## pypdf - A Custom Approach\n",
    "\n",
    "`pip install pypdf`\n",
    "\n",
    "In [__Part 1__](RateLimiting_CompaniesHouse_Part1.ipynb) of this article, a custom approach was built using `pypdf`.\n",
    "\n",
    "As part of the data extraction process, these functions were created that are useful in general for this dataset so have been repeated here:\n",
    "- `column_labels_from_first_row()` sets the DataFrame column labels from the first row and then filters out that row.\n",
    "- The table data is split across several pages and this produces fragmentary output that caa be recombined using `reconstruct_split_rows()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43b0041-73ac-42e2-bea1-61549ad548ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_labels_from_first_row(df):\n",
    "    \n",
    "    # set the column labels from the row 0\n",
    "    df.columns = df.iloc[0]\n",
    "    \n",
    "    # filter out row 0\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    # reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def row_has_nans(df):\n",
    "    return df.isna().any(axis=1)\n",
    "\n",
    "\n",
    "def reconstruct_split_rows(df, split_row_filter=row_has_nans):\n",
    "\n",
    "    # calculate a row_id \n",
    "    row_id = (~split_row_filter(df)).cumsum()\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        # fill any nulls with an empty string\n",
    "        .fillna(\"\")\n",
    "        # group by the row id\n",
    "        .groupby(row_id)\n",
    "        # aggregate the data by joining the strings\n",
    "        .agg(lambda x: \"\".join(x))\n",
    "        # reset the index\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72586660-6329-4fca-8908-166462a58740",
   "metadata": {},
   "source": [
    "Overall, using `pypdf` was a complex process but delivered a perfect representation of the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0bdbb13c-8261-41dc-99fb-b070d410c1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <th>Name of guidelines/principles</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Country of issuer</th>\n",
       "      <th>Type of issuer</th>\n",
       "      <th>Date of publishing</th>\n",
       "      <th>Target audience</th>\n",
       "      <th>Retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of robotics</td>\n",
       "      <td>Principles for designers, builders and users o...</td>\n",
       "      <td>Engineering and Physical Sciences Research Cou...</td>\n",
       "      <td>UK</td>\n",
       "      <td>Science foundation</td>\n",
       "      <td>1-Apr-2011</td>\n",
       "      <td>multiple (public, developers)</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethique de la recherche en robotique</td>\n",
       "      <td>Pr√©conisations</td>\n",
       "      <td>CERNA (Allistene)</td>\n",
       "      <td>France</td>\n",
       "      <td>Research alliance</td>\n",
       "      <td>xx-Nov-2014</td>\n",
       "      <td>researchers</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Ethical Frame for Big Data Analysis. I...</td>\n",
       "      <td>Values for an Ethical Frame</td>\n",
       "      <td>The Information Accountability Foundation</td>\n",
       "      <td>USA</td>\n",
       "      <td>NPO/Charity</td>\n",
       "      <td>xx-Mar-2015</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics Policy</td>\n",
       "      <td>IIIM's Ethics Policy</td>\n",
       "      <td>Icelandic Institute for Intelligent Machines (...</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>31-Aug-2015</td>\n",
       "      <td>self</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The AI Now Report. The Social and Economic Imp...</td>\n",
       "      <td>Key recommendations</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>USA</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>22-Sep-2016</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name of Document/Website  \\\n",
       "0                             Principles of robotics   \n",
       "1               Ethique de la recherche en robotique   \n",
       "2  Unified Ethical Frame for Big Data Analysis. I...   \n",
       "3                                      Ethics Policy   \n",
       "4  The AI Now Report. The Social and Economic Imp...   \n",
       "\n",
       "                       Name of guidelines/principles  \\\n",
       "0  Principles for designers, builders and users o...   \n",
       "1                                     Pr√©conisations   \n",
       "2                        Values for an Ethical Frame   \n",
       "3                               IIIM's Ethics Policy   \n",
       "4                                Key recommendations   \n",
       "\n",
       "                                              Issuer Country of issuer  \\\n",
       "0  Engineering and Physical Sciences Research Cou...                UK   \n",
       "1                                  CERNA (Allistene)            France   \n",
       "2          The Information Accountability Foundation               USA   \n",
       "3  Icelandic Institute for Intelligent Machines (...           Iceland   \n",
       "4                                   AI Now Institute               USA   \n",
       "\n",
       "                      Type of issuer Date of publishing  \\\n",
       "0                 Science foundation         1-Apr-2011   \n",
       "1                  Research alliance        xx-Nov-2014   \n",
       "2                        NPO/Charity        xx-Mar-2015   \n",
       "3  Academic and research institution        31-Aug-2015   \n",
       "4  Academic and research institution        22-Sep-2016   \n",
       "\n",
       "                 Target audience          Retrieval  \n",
       "0  multiple (public, developers)           Linkhubs  \n",
       "1                    researchers  Citation chaining  \n",
       "2                    unspecified  Citation chaining  \n",
       "3                           self           Linkhubs  \n",
       "4                    unspecified  Citation chaining  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# load part 1 form a csv file\n",
    "df_pypdf = pandas.read_csv(PDF.with_suffix(\".csv\"))\n",
    "\n",
    "df_pypdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13f2c5-7710-4c03-bd6c-3a262f0aaddf",
   "metadata": {},
   "source": [
    "## Adobe\n",
    "\n",
    "`pip install pdfservices-sdk`\n",
    "\n",
    "Adobe may have gifted the PDF format to the world but _\"what the right hand gives, the left hand takes\"_ and Adobe kept the format proprietary until 2008, controlling many of the most important PDF features including editing and manipulation. \n",
    "Today, Adobe offers a powerful toolchain for working with PDFs but this is accessed through an API that is usage tracked against user accounts.\n",
    "Adobe's API tools include document hosting (free unlimited); document generation; document signing; and (most-relevant-here) text-extraction.\n",
    "The text extract API combines Adobe's extensive experience with the PDF format with machine learning and natural language processing techniques.\n",
    "The API has python bindings but the text extract API is capped at __500 requests per month__ for the free tier.\n",
    "\n",
    "The __API setup__ is helpfully described online in a [quickstart guide](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/quickstarts/python/). \n",
    "Once registering you account with Adobe, navigating to https://developer.adobe.com/document-services yields a prominent __Get credentials__ button on the menu.\n",
    "Clicking this opens up a new site with a choice between PDF Services API (what we need here) and PDF Embed API (free unlimited for hosting PDF).\n",
    "Clicking create credentials on PDF Services API navigates to a page where you can specify the name for your credentials and select your language of choice. \n",
    "Selecting python, there is a checkbox option to generate language-specific code samples that demonstrate how the API can be used.\n",
    "N.B. Your authentication credentials are supplied to you in a `.json` file - this should be protected as sensitive information and accessed using e.g. environment variables.\n",
    "Your project credentials and usage statistics are also available in the [project dashboard](https://developer.adobe.com/console/projects).\n",
    "\n",
    "\n",
    "This code is derived from the personalised code sample that provided upon setting up the application at Adobe.\n",
    "The first thing to note is that Adobe's API relies heavily on chaining methods, often found in e.g. Java. \n",
    "\n",
    "- To protect against using all my allowance, the code here first checks if the result has already been calculated. The result is returned from Adobe as a zip file that I have chose to save as the same name as the original PDF file but with the `.zip` suffix.  \n",
    "- To interface Adobe's API, the relevant credentials need to me exchanged. `credentials` is created as an instance of `Credentials` using the builder `.service_principal_credentials_builder()`, with the appropriate information supplied using the `.with_client_id()` and `.with_client_secret()` chained methods, before being `.built()`. The secret credentials are protected by storing as environment variables that can be retrieved using the [`dotenv` package](https://pypi.org/project/python-dotenv/). \n",
    "- An `ExecutionContext`: `context` is created from the `credentials`.\n",
    "- The desired `operation` is initialised using `ExtractPDFOperation.create_new()`. \n",
    "- The local path of the file is supplied to the operation by creating `source` as a `FileRef` object describing the local PDF file's full path provided from the `.resolve()` of the `pathlib.Path` object. - The `source` is then registered against the `operation` using the `.set_source()` method.\n",
    "- The final stage of the setup is to describe the operation type in `options`. This is an `ExtractPDFOptions` object created using the `.builder()` method with the desired text element type: `ExtractElementType.TEXT` and `ExtractElementType.TABLES` added using the `.with_element_to_extract()` chained method, before being `.built()`.\n",
    "- The options are registered against the `operation` using the `.set_options()` method.\n",
    "- The result is generated form the operation and context `operation.execute(context)` and this is saved locally `.save_as()` as a `.zip` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b568964-d590-4af9-86a7-f9a629eeca65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import json\n",
    "\n",
    "from adobe.pdfservices.operation.auth.credentials import Credentials\n",
    "from adobe.pdfservices.operation.execution_context import ExecutionContext\n",
    "from adobe.pdfservices.operation.pdfops.extract_pdf_operation import ExtractPDFOperation\n",
    "from adobe.pdfservices.operation.io.file_ref import FileRef\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_pdf_options import ExtractPDFOptions\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_element_type import ExtractElementType\n",
    "\n",
    "ZIP = PDF.with_suffix(\".zip\")\n",
    "\n",
    "if not ZIP.is_file():\n",
    "\n",
    "    # setup access credentials\n",
    "    credentials = (\n",
    "        # base object and builder\n",
    "        Credentials.service_principal_credentials_builder()\n",
    "        # register user data\n",
    "        .with_client_id(dotenv.dotenv_values()['adobe_client_id'])\n",
    "        .with_client_secret(dotenv.dotenv_values()['adobe_client_secret'])\n",
    "        # build\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # set the context\n",
    "    context = ExecutionContext.create(credentials)\n",
    "\n",
    "    # choose the operation\n",
    "    operation = ExtractPDFOperation.create_new()\n",
    "\n",
    "    # choose the file source\n",
    "    source = FileRef.create_from_local_file(PDF.resolve())\n",
    "    operation.set_input(source)\n",
    "\n",
    "    # use optons to select the element type to extract\n",
    "    options = (\n",
    "        # base object and builder\n",
    "        ExtractPDFOptions.builder()\n",
    "        # select text and tables to extract\n",
    "        .with_element_to_extract(ExtractElementType.TEXT)\n",
    "        .with_element_to_extract(ExtractElementType.TABLES)\n",
    "        # build\n",
    "        .build()\n",
    "    )\n",
    "    operation.set_options(options)\n",
    "\n",
    "    # get result\n",
    "    result = operation.execute(context)\n",
    "\n",
    "    # save result ouput\n",
    "    result.save_as(ZIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590f259-9ab1-4392-9300-9e859c34f500",
   "metadata": {},
   "source": [
    "The result `.zip` file contains a `.json` file called `'structuredData.json'` that holds the result of the extraction.\n",
    "This is a large `.json` file contains details of all the elements within the PDF referenced by the `\"elements\"` key. \n",
    "Adobe parsed the PDF to include a document path for each element, for example: `\"//Document/Table/TR/TD/P\"`.\n",
    "This is similar to a xml XPath or the HTML location of the element if the data were presented as a website. \n",
    "\n",
    "However, Adobe was tasked with extracting with the `ExtractElementType.TABLES` option too. \n",
    "Within the result `.zip` file there is a folder `tables/` containing all the tables that Adobe could construct, i.e. it did all the hard work for us.\n",
    "The relevant table is `tables/fileoutpart1.xlsx` and this can be loaded into a DataFrame using `pandas.read_excel()`\n",
    "Again, the split data needs recombining. There are also many `_x000D_` that have been added as a byproduct of excel creation.\n",
    "These can be removed by applying the `openpyxl.utils.escape.unescape` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b710849-d831-40c2-a335-4b9a264dbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <th>Name of guidelines/principl es</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Country of issuer</th>\n",
       "      <th>Type of issuer</th>\n",
       "      <th>Date of publishi ng</th>\n",
       "      <th>Target audience</th>\n",
       "      <th>Retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of robotics</td>\n",
       "      <td>Principles for designers, builders and users o...</td>\n",
       "      <td>Engineering and Physical Sciences Research Cou...</td>\n",
       "      <td>UK</td>\n",
       "      <td>Science foundation</td>\n",
       "      <td>1-Apr-2011</td>\n",
       "      <td>multiple (public, developers)</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethique de la recherche en robotique</td>\n",
       "      <td>Pr√©conisations</td>\n",
       "      <td>CERNA (Allistene)</td>\n",
       "      <td>France</td>\n",
       "      <td>Research alliance</td>\n",
       "      <td>xx-Nov-2014</td>\n",
       "      <td>researchers</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Ethical Frame for Big Data Analysis. I...</td>\n",
       "      <td>Values for an Ethical Frame</td>\n",
       "      <td>The Information Accountability Foundation</td>\n",
       "      <td>USA</td>\n",
       "      <td>NPO/Charity</td>\n",
       "      <td>xx-Mar-2015</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics Policy</td>\n",
       "      <td>IIIM's Ethics Policy</td>\n",
       "      <td>Icelandic Institute for Intelligent Machines \\...</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>31-Aug-2015</td>\n",
       "      <td>self</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The AI Now Report. The Social and Economic Imp...</td>\n",
       "      <td>Key recommendations</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>USA</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>22-Sep-2016</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name of Document/Website  \\\n",
       "0                             Principles of robotics   \n",
       "1               Ethique de la recherche en robotique   \n",
       "2  Unified Ethical Frame for Big Data Analysis. I...   \n",
       "3                                      Ethics Policy   \n",
       "4  The AI Now Report. The Social and Economic Imp...   \n",
       "\n",
       "                      Name of guidelines/principl es  \\\n",
       "0  Principles for designers, builders and users o...   \n",
       "1                                     Pr√©conisations   \n",
       "2                        Values for an Ethical Frame   \n",
       "3                               IIIM's Ethics Policy   \n",
       "4                                Key recommendations   \n",
       "\n",
       "                                              Issuer Country of issuer  \\\n",
       "0  Engineering and Physical Sciences Research Cou...                UK   \n",
       "1                                  CERNA (Allistene)            France   \n",
       "2          The Information Accountability Foundation               USA   \n",
       "3  Icelandic Institute for Intelligent Machines \\...           Iceland   \n",
       "4                                   AI Now Institute               USA   \n",
       "\n",
       "                      Type of issuer Date of publishi ng  \\\n",
       "0                 Science foundation          1-Apr-2011   \n",
       "1                  Research alliance         xx-Nov-2014   \n",
       "2                        NPO/Charity         xx-Mar-2015   \n",
       "3  Academic and research institution         31-Aug-2015   \n",
       "4  Academic and research institution         22-Sep-2016   \n",
       "\n",
       "                 Target audience          Retrieval  \n",
       "0  multiple (public, developers)           Linkhubs  \n",
       "1                    researchers  Citation chaining  \n",
       "2                    unspecified  Citation chaining  \n",
       "3                           self           Linkhubs  \n",
       "4                    unspecified  Citation chaining  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import openpyxl\n",
    "\n",
    "with zipfile.ZipFile(ZIP, 'r') as z:\n",
    "    df_adobe_auto = pandas.read_excel(io.BytesIO(z.read('tables/fileoutpart1.xlsx')))\n",
    "\n",
    "# reconstruct split rows    \n",
    "df_adobe_auto = reconstruct_split_rows(df_adobe_auto)\n",
    "    \n",
    "# remove import remnants\n",
    "df_adobe_auto.columns = pandas.Series(df_adobe_auto.columns).apply(openpyxl.utils.escape.unescape).str.strip()\n",
    "for label in df_adobe_auto:\n",
    "    df_adobe_auto[label] = df_adobe_auto[label].astype(str).apply(openpyxl.utils.escape.unescape).str.strip()\n",
    "    \n",
    "# look at the first 5 rows\n",
    "df_adobe_auto.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07782470-e92b-4c47-b2f8-32d7a63dbc9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pdfplumber\n",
    "\n",
    "`pip install pdfplumber`\n",
    "\n",
    "pdfplumber is a [python package](https://pypi.org/project/pdfplumber/) that captures all text characters from within a PDF with detailed information about their layout along with any rectangles and lines on the page. \n",
    "pdfplumber is able to use all this information to extract tabular data.\n",
    "pdfplumber is built upon the the [`pdfminer` package](https://pypi.org/project/pdfminer/) and is __pure python__.\n",
    "\n",
    "The process for extracting our tabular data is very streamlined in this dedicated package:\n",
    "- A pdfplumber `pdf.PDF` object is opened using the `.open()` method with the path of the file, then the target pages `[3:8]` are accessible by indexing the `.pages` property.\n",
    "- Each page has an `.extract_table()` method that produces tabular data as a list that can be converted into a `pandas.DataFrame` and collected in a list `pdfplumber_tables`.\n",
    "- All the information can be combined into a single DataFrame using `pandas.concat()`.\n",
    "- The column labels are collected from the first row using the `column_labels_from_first_row()` function.\n",
    "- Data that was split across page breaks is recombined using the `reconstruct_split_rows()` function. Here, the null data is not `numpy.nan` but empty `str` so needs to be found using a lambda function that returns the number of items having `.len()` of `0`: `.apply(lambda row: (row.str.len() == 0).sum(), axis=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3cfd3825-6dfc-47e5-83b5-1c4b6eac5b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <th>Name of guidelines/principl es</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Country of issuer</th>\n",
       "      <th>Type of issuer</th>\n",
       "      <th>Date of publishi ng</th>\n",
       "      <th>Target audience</th>\n",
       "      <th>Retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of robotics</td>\n",
       "      <td>Principles for designers, builders and users o...</td>\n",
       "      <td>Engineering and Physical Sciences Research Cou...</td>\n",
       "      <td>UK</td>\n",
       "      <td>Science foundation</td>\n",
       "      <td>1-Apr- 2011</td>\n",
       "      <td>multiple (public, developers)</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethique de la recherche en robotique</td>\n",
       "      <td>Pr√©conisations</td>\n",
       "      <td>CERNA (Allistene)</td>\n",
       "      <td>France</td>\n",
       "      <td>Research alliance</td>\n",
       "      <td>xx-Nov- 2014</td>\n",
       "      <td>researchers</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Ethical Frame for Big Data Analysis. I...</td>\n",
       "      <td>Values for an Ethical Frame</td>\n",
       "      <td>The Information Accountability Foundation</td>\n",
       "      <td>USA</td>\n",
       "      <td>NPO/Charity</td>\n",
       "      <td>xx-Mar- 2015</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics Policy</td>\n",
       "      <td>IIIM's Ethics Policy</td>\n",
       "      <td>Icelandic Institute for Intelligent Machines (...</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>31-Aug- 2015</td>\n",
       "      <td>self</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The AI Now Report. The Social and Economic Imp...</td>\n",
       "      <td>Key recommendations</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>USA</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>22-Sep- 2016</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                           Name of Document/Website  \\\n",
       "0                             Principles of robotics   \n",
       "1               Ethique de la recherche en robotique   \n",
       "2  Unified Ethical Frame for Big Data Analysis. I...   \n",
       "3                                      Ethics Policy   \n",
       "4  The AI Now Report. The Social and Economic Imp...   \n",
       "\n",
       "0                     Name of guidelines/principl es  \\\n",
       "0  Principles for designers, builders and users o...   \n",
       "1                                     Pr√©conisations   \n",
       "2                        Values for an Ethical Frame   \n",
       "3                               IIIM's Ethics Policy   \n",
       "4                                Key recommendations   \n",
       "\n",
       "0                                             Issuer Country of issuer  \\\n",
       "0  Engineering and Physical Sciences Research Cou...                UK   \n",
       "1                                  CERNA (Allistene)            France   \n",
       "2          The Information Accountability Foundation               USA   \n",
       "3  Icelandic Institute for Intelligent Machines (...           Iceland   \n",
       "4                                   AI Now Institute               USA   \n",
       "\n",
       "0                     Type of issuer Date of publishi ng  \\\n",
       "0                 Science foundation         1-Apr- 2011   \n",
       "1                  Research alliance        xx-Nov- 2014   \n",
       "2                        NPO/Charity        xx-Mar- 2015   \n",
       "3  Academic and research institution        31-Aug- 2015   \n",
       "4  Academic and research institution        22-Sep- 2016   \n",
       "\n",
       "0                Target audience          Retrieval  \n",
       "0  multiple (public, developers)           Linkhubs  \n",
       "1                    researchers  Citation chaining  \n",
       "2                    unspecified  Citation chaining  \n",
       "3                           self           Linkhubs  \n",
       "4                    unspecified  Citation chaining  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# initialise a list to collect tables\n",
    "pdfplumber_tables = list()\n",
    "\n",
    "# get the tables from each page 3:8\n",
    "for page in pdfplumber.open(PDF).pages[3:8]:\n",
    "    df = pandas.DataFrame(page.extract_table())\n",
    "    pdfplumber_tables.append(df)\n",
    "\n",
    "# concat the dataframes on each page into one dataframe\n",
    "df_pdfplumber = pandas.concat(pdfplumber_tables)\n",
    "\n",
    "# strip the leading and trailing spaces\n",
    "df_pdfplumber = df_pdfplumber.apply(lambda series: series.str.strip())\n",
    "\n",
    "# convert new lines to spaces\n",
    "df_pdfplumber = df_pdfplumber.replace(r\"\\n\", \" \", regex=True)\n",
    "\n",
    "# set the headings to the first row\n",
    "df_pdfplumber = column_labels_from_first_row(df_pdfplumber)\n",
    "\n",
    "# recombine split rows\n",
    "df_pdfplumber = reconstruct_split_rows(df_pdfplumber,  lambda df: df.apply(lambda row: (row.str.len() == 0).sum(), axis=1) > 3)\n",
    "\n",
    "# look at the top 5 rows\n",
    "df_pdfplumber.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597b2ff-4ba2-4535-8f95-ff9a5c5d57ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tabula\n",
    "\n",
    "`pip install tabula-py` \n",
    "\n",
    "The [tabula-py python package](https://pypi.org/project/tabula-py/) is a python wrapper for [tabula java](https://github.com/tabulapdf/tabula-java).\n",
    "tabula is a set of tools for extracting tabular data from a PDF file and has been implemented as [installable software](https://tabula.technology/).\n",
    "Using tabula in python requires access to a __java__ installation. For me, Google colab resolved this automagically but I had some difficulty locally, experiencing inconsistent errors on import and having to manually modify system environment variables so that the package could find my java installation. \n",
    "\n",
    "Once everything was set up, the process for extracting our tabular data in tabula is simple:\n",
    "- tabula's `.read_pdf()` function creates the PDF object. \n",
    "The `pages=` argument targets our desired data;\n",
    "the encoding `encoding=` can be supplied as `\"windows-1252\"`;\n",
    "and `multiple_tables=` should be `False` because Table S2 is a single table spread across multiple pages.\n",
    "- The output of this function is a list of `pandas.DataFrame` of length 1 because `multiple_tables=` was `False`. \n",
    "- The column labels were already found correctly.\n",
    "- Other outputs had information split across 3 extra rows where the PDF had page breaks but this data is much more fragmented with __334__ total rows. However, this data can be combined in exactly the same way using `reconstruct_split_rows()`.\n",
    "- All the multi-line was separated by a `\"\\r\"` line break - these could be replaced with spaces using the appropriate `.replace()` methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e2264565-cbb8-4d12-9ef1-9fe639679e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <th>Name of guidelines/principl es</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Country of issuer</th>\n",
       "      <th>Type of issuer</th>\n",
       "      <th>Date of publishi ng</th>\n",
       "      <th>Target audience</th>\n",
       "      <th>Retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of robotics</td>\n",
       "      <td>Principles fordesigners, buildersand users of ...</td>\n",
       "      <td>Engineering andPhysical SciencesResearch Counc...</td>\n",
       "      <td>UK</td>\n",
       "      <td>Sciencefoundation</td>\n",
       "      <td>1-Apr-2011</td>\n",
       "      <td>multiple (public,developers)</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethique de larecherche en robotique</td>\n",
       "      <td>Pr√©conisations</td>\n",
       "      <td>CERNA (Allistene)</td>\n",
       "      <td>France</td>\n",
       "      <td>Research alliance</td>\n",
       "      <td>xx-Nov-2014</td>\n",
       "      <td>researchers</td>\n",
       "      <td>Citationchaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Ethical Framefor Big Data Analysis.IAF...</td>\n",
       "      <td>Values for anEthical Frame</td>\n",
       "      <td>The InformationAccountabilityFoundation</td>\n",
       "      <td>USA</td>\n",
       "      <td>NPO/Charity</td>\n",
       "      <td>xx-Mar-2015</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citationchaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics Policy</td>\n",
       "      <td>IIIM's Ethics Policy</td>\n",
       "      <td>Icelandic Institute forIntelligent Machines(IIIM)</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Academic andresearchinstitution</td>\n",
       "      <td>31-Aug-2015</td>\n",
       "      <td>self</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The AI Now Report.The Social andEconomicImplic...</td>\n",
       "      <td>Keyrecommendations</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>USA</td>\n",
       "      <td>Academic andresearchinstitution</td>\n",
       "      <td>22-Sep-2016</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citationchaining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name of Document/Website  \\\n",
       "0                             Principles of robotics   \n",
       "1                Ethique de larecherche en robotique   \n",
       "2  Unified Ethical Framefor Big Data Analysis.IAF...   \n",
       "3                                      Ethics Policy   \n",
       "4  The AI Now Report.The Social andEconomicImplic...   \n",
       "\n",
       "                      Name of guidelines/principl es  \\\n",
       "0  Principles fordesigners, buildersand users of ...   \n",
       "1                                     Pr√©conisations   \n",
       "2                         Values for anEthical Frame   \n",
       "3                               IIIM's Ethics Policy   \n",
       "4                                 Keyrecommendations   \n",
       "\n",
       "                                              Issuer Country of issuer  \\\n",
       "0  Engineering andPhysical SciencesResearch Counc...                UK   \n",
       "1                                  CERNA (Allistene)            France   \n",
       "2            The InformationAccountabilityFoundation               USA   \n",
       "3  Icelandic Institute forIntelligent Machines(IIIM)           Iceland   \n",
       "4                                   AI Now Institute               USA   \n",
       "\n",
       "                    Type of issuer Date of publishi ng  \\\n",
       "0                Sciencefoundation          1-Apr-2011   \n",
       "1                Research alliance         xx-Nov-2014   \n",
       "2                      NPO/Charity         xx-Mar-2015   \n",
       "3  Academic andresearchinstitution         31-Aug-2015   \n",
       "4  Academic andresearchinstitution         22-Sep-2016   \n",
       "\n",
       "                Target audience         Retrieval  \n",
       "0  multiple (public,developers)          Linkhubs  \n",
       "1                   researchers  Citationchaining  \n",
       "2                   unspecified  Citationchaining  \n",
       "3                          self          Linkhubs  \n",
       "4                   unspecified  Citationchaining  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tabula\n",
    "\n",
    "# load the tables from the target page range\n",
    "tabula_tables = tabula.read_pdf(PDF, pages=\"4-8\", encoding=\"windows-1252\", multiple_tables=False)\n",
    "\n",
    "# recombine split rows\n",
    "df_tabula = reconstruct_split_rows(tabula_tables[0])\n",
    "\n",
    "# remove \\r from data\n",
    "df_tabula.columns = map(lambda s:s.replace('\\r', ' '), df_tabula.columns)\n",
    "df_tabula = df_tabula.replace(r'\\r', '', regex=True)\n",
    "\n",
    "# inspect the top 5 rows\n",
    "df_tabula.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4482d-d985-465c-89bf-2e599991799f",
   "metadata": {},
   "source": [
    "## camelot\n",
    "\n",
    "`pip install camelot-py`\n",
    "\n",
    "[camelot](https://camelot-py.readthedocs.io/) is another [package](https://pypi.org/project/camelot-py/) that can be used to extract tabular PDF data and it has a suitably named accompanying web interface [excalibur](https://excalibur-py.readthedocs.io/). \n",
    "The camelot name comes from the [code name of the project](https://en.wikipedia.org/wiki/History_of_PDF) that was the progenitor of PDF itself.\n",
    "The main [advantage](https://camelot-py.readthedocs.io/en/master/user/intro.html#why-another-pdf-table-extraction-library) of camelot over other PDF extraction packages is that it provides easy access to metadata and fine-tuning that can help when extraction is not straightforward. camelot requires __ghostscript__ and __Tkinter__ to be installed on your system and this can be a bit of a fiddle to set up depending on your environment.\n",
    "camelot is built upon the the [`pdfminer` package](https://pypi.org/project/pdfminer/).\n",
    "\n",
    "\n",
    "The process for extracting our tabular data in camelot is:\n",
    "- camelot's `.read_pdf()` function creates the PDF object using the `pages=` argument to target our desired data; and `strip_text=\"\\n\"` so that multi-line data is joined without including the newline character.\n",
    "- Separate DataFrame objects are accessible from each table's `.df` property and these can be combined using `pandas.concat()`.\n",
    "- The column labels are collected from the first row.\n",
    "- Data that was split across page breaks is recombined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a6d9d03e-d01e-4015-a1e7-8ac1daeaa75a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ame of Document/Website</th>\n",
       "      <th>Name of guidelines/principles</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Country of issuer</th>\n",
       "      <th>Type of issuer</th>\n",
       "      <th>Date of publishing</th>\n",
       "      <th>Target audience</th>\n",
       "      <th>Retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of robotics</td>\n",
       "      <td>Principles for designers, builders and users o...</td>\n",
       "      <td>Engineering and Physical Sciences Research Cou...</td>\n",
       "      <td>UK</td>\n",
       "      <td>Science foundation</td>\n",
       "      <td>1-Apr-2011</td>\n",
       "      <td>multiple (public, developers)</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethique de la recherche en robotique</td>\n",
       "      <td>Pr√©conisations</td>\n",
       "      <td>CERNA (Allistene)</td>\n",
       "      <td>France</td>\n",
       "      <td>Research alliance</td>\n",
       "      <td>xx-Nov-2014</td>\n",
       "      <td>researchers</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Ethical Frame for Big Data Analysis. I...</td>\n",
       "      <td>Values for an Ethical Frame</td>\n",
       "      <td>The Information Accountability Foundation</td>\n",
       "      <td>USA</td>\n",
       "      <td>NPO/Charity</td>\n",
       "      <td>xx-Mar-2015</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics Policy</td>\n",
       "      <td>IIIM's Ethics Policy</td>\n",
       "      <td>Icelandic Institute for Intelligent Machines (...</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>31-Aug-2015</td>\n",
       "      <td>self</td>\n",
       "      <td>Linkhubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The AI Now Report. The Social and Economic Imp...</td>\n",
       "      <td>Key recommendations</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>USA</td>\n",
       "      <td>Academic and research institution</td>\n",
       "      <td>22-Sep-2016</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>Citation chaining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                            ame of Document/Website  \\\n",
       "0                             Principles of robotics   \n",
       "1               Ethique de la recherche en robotique   \n",
       "2  Unified Ethical Frame for Big Data Analysis. I...   \n",
       "3                                      Ethics Policy   \n",
       "4  The AI Now Report. The Social and Economic Imp...   \n",
       "\n",
       "0                      Name of guidelines/principles  \\\n",
       "0  Principles for designers, builders and users o...   \n",
       "1                                     Pr√©conisations   \n",
       "2                        Values for an Ethical Frame   \n",
       "3                               IIIM's Ethics Policy   \n",
       "4                                Key recommendations   \n",
       "\n",
       "0                                             Issuer Country of issuer  \\\n",
       "0  Engineering and Physical Sciences Research Cou...                UK   \n",
       "1                                  CERNA (Allistene)            France   \n",
       "2          The Information Accountability Foundation               USA   \n",
       "3  Icelandic Institute for Intelligent Machines (...           Iceland   \n",
       "4                                   AI Now Institute               USA   \n",
       "\n",
       "0                     Type of issuer Date of publishing  \\\n",
       "0                 Science foundation         1-Apr-2011   \n",
       "1                  Research alliance        xx-Nov-2014   \n",
       "2                        NPO/Charity        xx-Mar-2015   \n",
       "3  Academic and research institution        31-Aug-2015   \n",
       "4  Academic and research institution        22-Sep-2016   \n",
       "\n",
       "0                Target audience          Retrieval  \n",
       "0  multiple (public, developers)           Linkhubs  \n",
       "1                    researchers  Citation chaining  \n",
       "2                    unspecified  Citation chaining  \n",
       "3                           self           Linkhubs  \n",
       "4                    unspecified  Citation chaining  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import camelot\n",
    "\n",
    "# load tables from pages 4-8\n",
    "camelot_tables = camelot.read_pdf(str(PDF), pages='4-8', strip_text=\"\\n\")\n",
    "\n",
    "# concat each page's df into one dataframe\n",
    "df_camelot = pandas.concat(table.df for table in camelot_tables)\n",
    "\n",
    "# get column labels from first row\n",
    "df_camelot = column_labels_from_first_row(df_camelot)\n",
    "\n",
    "# recombine split rows\n",
    "df_camelot = reconstruct_split_rows(df_camelot, lambda df: df.apply(lambda row: (row.str.len() == 0).sum(), axis=1) > 3)\n",
    "\n",
    "# look at the first 5 rows\n",
    "df_camelot.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86978e-fe43-43a4-bf80-22fbabf4c58e",
   "metadata": {},
   "source": [
    "## pdfminer, PyMuPDF and pdfquery\n",
    "These are other popular PDF packages for python but were beyond the scope of this document because I couldn't find a fast way to extract quality tabular data using them.\n",
    "Feel free to correct me and I will update this document!\n",
    "\n",
    "`pip install pdfminer`\n",
    "- [`pdfminer` is a powerful package](https://pypi.org/project/pdfminer/) that `pdfplumber` and `camelot` are both built upon.\n",
    "\n",
    "\n",
    "`pip install PyMuPDF`\n",
    "- [`PyMuPDF` is a popular PDF extraction](https://pypi.org/project/PyMuPDF/) tool but I couldn't shape it into getting valuable data without having to invest significant efforts.\n",
    "\n",
    "`pip install pdfquery`\n",
    "- [`pdfquery` is a package](https://pypi.org/project/pdfquery/) that creates an XPath-type tree for the PDF document allowing for powerful queries to be executed on the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18a2d0-5928-4748-9fb7-5b186bbf0298",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OCR\n",
    "When extracting text from a PDF there is a further complication that, in addition to fragmentary text vector data littering the page, text can be stored as a rasterised images within the PDF.\n",
    "This results in an oft-used approach to extracting PDF data is processing the whole PDF as an image and then extracting relevant data using OCR (optical character recognition)!\n",
    "`pytesseract` is a popular [python OCR package](https://pypi.org/project/pytesseract/) if you face this challenge and there is a plethora of online offerings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeaac38-63c0-44ed-9f88-0feb6b1142b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comparison\n",
    "\n",
    "The quality of the output from `pypdf` was checked manually against the PDF data source and was found to be 100% accurate. \n",
    "To measure the performance of the other libraries the [Levenshtein edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance) can be calculated against the pypdf output `df_pypdf`.\n",
    "The [natural language toolkit NLTK](https://www.nltk.org/) has tools for [measuring the distance](https://www.nltk.org/api/nltk.metrics.distance.html#module-nltk.metrics.distance) between two strings.\n",
    "\n",
    "Each of the data sources is gathered into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf5ed3f3-a489-4167-bb09-cafdc1500ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = {\n",
    "    \"pypdf\": df_pypdf,\n",
    "    \"adobe\": df_adobe_auto,\n",
    "    \"pdfplumber\": df_pdfplumber,\n",
    "    \"tabula\": df_tabula,\n",
    "    \"camelot\": df_camelot,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba1e81-e5c3-43c1-8c3c-526bdb3385cb",
   "metadata": {},
   "source": [
    "A DataFrame can be constructed that contains the Levenshtein edit distance for the values in the column labels and for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bd2a1313-8034-446c-aa7c-74ecb173e6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pypdf</th>\n",
       "      <th>adobe</th>\n",
       "      <th>pdfplumber</th>\n",
       "      <th>tabula</th>\n",
       "      <th>camelot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Column labels</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name of guidelines/principles</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country of issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type of issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date of publishing</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target audience</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrieval</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               pypdf  adobe  pdfplumber  tabula  camelot\n",
       "Column labels                      0      2           2       2        1\n",
       "Name of Document/Website           0      4           2      73        0\n",
       "Name of guidelines/principles      0      4           2      58        0\n",
       "Issuer                             0      9           1      45        0\n",
       "Country of issuer                  0     19          19       5        0\n",
       "Type of issuer                     0     30          34      44        0\n",
       "Date of publishing                 0      2          76       5        0\n",
       "Target audience                    0      6           1      42        0\n",
       "Retrieval                          0      1          15      58        1"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for name, df in outputs.items():\n",
    "    scores[name] = {\"Column labels\": edit_distance(df_pypdf.columns, df.columns)}\n",
    "    \n",
    "    for column_index, column_label in enumerate(df_pypdf.columns):\n",
    "        scores[name][column_label] = edit_distance(\n",
    "            df_pypdf.iloc[:, column_index].fillna(\"\"),\n",
    "            df.iloc[:, column_index].fillna(\"\"),\n",
    "        )\n",
    "\n",
    "pandas.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adaa97-706e-4be0-8bac-b60b784219b5",
   "metadata": {},
   "source": [
    "However, many of the difference are due to whitespace inconsistency, _e.g._ extra and missing spaces.\n",
    "If all the whitespace is removed then the result is a much closer Levenshtein edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1082fa3a-4425-4d96-af73-6efbc6407ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pypdf</th>\n",
       "      <th>adobe</th>\n",
       "      <th>pdfplumber</th>\n",
       "      <th>tabula</th>\n",
       "      <th>camelot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Column labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name of Document/Website</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name of guidelines/principles</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country of issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type of issuer</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date of publishing</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target audience</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrieval</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               pypdf  adobe  pdfplumber  tabula  camelot\n",
       "Column labels                      0      0           0       0        1\n",
       "Name of Document/Website           0      2           0       4        0\n",
       "Name of guidelines/principles      0      2           0       6        0\n",
       "Issuer                             0      2           0       4        0\n",
       "Country of issuer                  0      2           0       4        0\n",
       "Type of issuer                     0      2           0       4        0\n",
       "Date of publishing                 0      2           0       4        0\n",
       "Target audience                    0      2           0       4        0\n",
       "Retrieval                          0      1           0       3        1"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = dict()\n",
    "\n",
    "for name, df in outputs.items():\n",
    "    scores[name] = {\"Column labels\": edit_distance(df_pypdf.columns.str.replace(\"\\s+\", \"\", regex=True), df.columns.str.replace(\"\\s+\", \"\", regex=True))}\n",
    "    \n",
    "    for column_index, column_label in enumerate(df_pypdf.columns):\n",
    "        scores[name][column_label] = edit_distance(\n",
    "            df_pypdf.iloc[:, column_index].fillna(\"\").str.replace(\"\\s+\", \"\", regex=True),\n",
    "            df.iloc[:, column_index].fillna(\"\").str.replace(\"\\s+\", \"\", regex=True),\n",
    "        )\n",
    "\n",
    "pandas.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691d574-7601-44f5-853d-5e38568364e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "There are over 5 trillion PDF files in the world and they hold a wealth of valuable information locked within their pages. \n",
    "Extracting tables from PDF documents can be a challenging task due to the complexity and variability of the PDF file format.\n",
    "Having spend considerable effort in creating a custom table extractor using `pypdf` in [__Part 1__](RateLimiting_CompaniesHouse_Part1.ipynb) of this article, \n",
    "and here in [__Part 2__](RateLimiting_CompaniesHouse_Part2.ipynb) we considered the state-of-the-art in python packages that are designed to streamline tabular data extraction.\n",
    "\n",
    "__Adobe__'s own API delivers table extraction and offers 500 requests per month at the free tier.\n",
    "There is the overhead of setting up the API and the requirement to transmit the PDF file to Adobe for processing but Adobe successfully delivered an excel file containing the table.\n",
    "__pdfplumber__ was the most straightforward package to use and delivered excellent results.\n",
    "__tabula__ had the complication of setting up a link between python and a Java backend. tabula delivered the data but this had the most errors and initially the data was spread across 250 additional rows before being combined.\n",
    "__camelot__ had the complication of installing ghostscript but delivered excellent results.\n",
    "\n",
    "\n",
    "It is important to note that rendering PDF documents as images followed by Optical Character Recognition (OCR) is a viable alternative for table extraction. There are many online tools to help this process and recent advances in computer vision make this a powerful approach that may even replace direct PDF extraction methods in certain scenarios.\n",
    "\n",
    "Overall, the choice of tool for extracting tables from PDFs should be guided by the specific requirements of the task.\n",
    "The range of formatting in PDFs means that some customisation will be required for the best results in every solution. \n",
    "For example, here most DataFrames needed some cleaning.\n",
    "Overall, there are many excellent python packages to help extract your data from PDF files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
